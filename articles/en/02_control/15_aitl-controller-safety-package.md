---
layout: default
title: zenn-articles
---

# ã€Controlã€‘ğŸ›¡ï¸ 15. (Safety Design) What Is a Safety Envelope?
## Designing the Boundary AI Control Must Never Cross
topics: ["control engineering", "AI", "safety design", "FSM", "anomaly detection"]

---

## âš ï¸ Introduction: The Most Dangerous Thing in AI Control Is â€œNo Boundaryâ€

In discussions about AI-based control, the most dangerous situation is this:

> *â€œNo one has clearly defined how far the AI is allowed to go.â€*

Poor performance is not the real risk.  
**The absence of a boundary is far more dangerous.**

This article explains **Safety Envelope**,  
the core concept of the **AI Control Safety Package**.

---

## ğŸ§± What Is a Safety Envelope?

In one sentence, a Safety Envelope is:

> **â€œThe operational boundary that AI must never violate.â€**

The critical point is this:

**A Safety Envelope is not decided by AI.**

---

### ğŸ“ What a Safety Envelope Defines

- Permissible state space  
- Allowed input/output ranges  
- Allowed transition rates  
- Allowed time constraints (dwell time, response delay)

These are **designed and fixed by humans**,  
intentionally limiting the degrees of freedom of AI.

---

## ğŸš« Why AI Must Not Define Its Own Boundary

AI systemsâ€”especially LLMsâ€”have the following properties:

- They generate statistically plausible outputs  
- They do not recognize failure as failure  
- They reinterpret boundary conditions on their own  

In other words:

> **You must never let the entity that is being judged for safety  
> define what â€œsafeâ€ means.**

---

## ğŸ§¯ A Safety Envelope Is Not a Performance Limiter

This is a common misunderstanding.

A Safety Envelope is **not** designed to restrict performance.

Its real purpose is:

- Stop before entering a dangerous region  
- Switch modes before physical damage occurs  
- Keep the system in a recoverable state  

It is **design insurance**, not optimization.

---

## ğŸ§© Fundamental Components of a Safety Envelope

### ğŸŸ¦ â‘  State Variable Selection

First, define what must be monitored:

- Physical variables (position, velocity, voltage, current)  
- Internal control states  
- Estimation errors  
- Variability or magnitude of AI outputs  

Not â€œobserve everything,â€ but  
**â€œobserve signs of failure.â€**

---

### ğŸŸ§ â‘¡ Boundary Definition

Next, define allowable limits:

- Hard limits that must never be crossed  
- Soft limits that trigger warnings  
- Time-dependent constraints  

Here, **conservatism is a virtue**.

---

### ğŸŸ¨ â‘¢ Deterministic Violation Detection

Detect approach or violation of the boundary:

- Threshold monitoring  
- Gradient / rate-of-change monitoring  
- Abnormal state transition detection  

The key rule:

> **Detection must be deterministic, not AI-driven.**

---

### ğŸŸ¥ â‘£ FSM-Based Supervisory Control

A Safety Envelope becomes effective only when paired with FSM.

Explicitly design states such as:

- Normal  
- Warning  
- Safe Mode  
- Shutdown  

These transitions are **designed, not inferred**.

---

## ğŸ”— Relationship to PID Ã— FSM Ã— LLM Architecture

### âš™ï¸ PID (Inner Layer)

- Operates only inside the Safety Envelope  
- Stability is guaranteed by PID design  

---

### ğŸ§¾ FSM (Supervisory Layer)

- Detects envelope violations  
- Forces mode transitions  

---

### ğŸ§  LLM (Outer Layer)

- Analyzes *why* a violation occurred  
- Proposes design improvements  

**LLMs do not define boundaries.  
They do not enforce them.  
They do not supervise them.**

---

## âŒ Common Mistakes

### ğŸš« Learning the Safety Envelope with AI

- Boundaries drift  
- Reproducibility is lost  
- Accountability disappears  

---

### ğŸš« Treating the Envelope as â€œReference Onlyâ€

- Violations do not stop the system  
- No one owns responsibility  

A Safety Envelope **must have enforcement power**.

---

## ğŸ§  Summary

- The Safety Envelope is the lifeline of AI control  
- Boundaries are designed by humans  
- Detection and switching are deterministic  
- AI supports only from the outside  

AI control is not dangerous by itself.  
**Failing to design boundaries is.**

---

## ğŸ”œ Next Article Preview

Next, we will cover:

> **â€œRecovery Control: How to Return Safely After Failure.â€**

In AI control systems,  
the real difference appears **after something goes wrong**.

---

## ğŸ“š References

- AI Control Safety Package  
  [https://samizo-aitl.github.io/ai-control-safety-package/](https://samizo-aitl.github.io/ai-control-safety-package/)

---

*End of Article*
